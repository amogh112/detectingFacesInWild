{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysing Face Detection on African data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import glob\n",
    "import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "from imutils import face_utils\n",
    "from skimage.feature import hog\n",
    "from skimage import data,exposure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib \n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import widgets, interactive, fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Path to Data Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_folder_bagamoyo_data = '/media/amogh/Stuff/CMU/datasets/bagamoyo_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Path to Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_folder_all_frames = path_folder_bagamoyo_data + '/bagamoyo_frames_all_in_one/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_folder_wise_frames = path_folder_bagamoyo_data + '/bagamoyo_frames_folder_wise/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Path to xml files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data-analysis.ipynb\t\t       train_dlib_detector.py\r\n",
      "Generate-xml-dlib.ipynb\t\t       training.xml\r\n",
      "README.md\t\t\t       ZFace label analysis.ipynb\r\n",
      "shape_predictor_68_face_landmarks.dat\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "comment: will change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_xml_file = 'training.xml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things to be calculated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Absolute - Find the number of frames in total in which face is detected, and how many in each frame\n",
    "2. Check continuity - a visualisation for seeing which frames in the continuity.\n",
    "3. This can be done by writing a single script which does these things if an xml file is generated with the name of the image and the coordinates of the bounding boxes in it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's take an example of training.xml and try to get some visualisations. \n",
    "\n",
    "1. Once the relevant visualisations can be obtained, then just generate an XML from every face detector's output and run the same script.\n",
    "2. Then you must be able to take these different plots and plot them in one.\n",
    "3. Or when plotting one, you should be able to take multiple XML and plot them on a single axis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parsing XML file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataframeFromXML(path_file_xml):\n",
    "    \"\"\"\n",
    "    Returns the dataframe(columns- videoName,frameNo,faceNo,left,right,width,height) from given xml file path holding bounding boxes for each frame.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path_file_xml : path of the XML file.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Pandas dataframe\n",
    "        Information about images and their boxes.\n",
    "    \"\"\"\n",
    "    \n",
    "    #make a new dataframe to store the data.\n",
    "    df = pd.DataFrame(columns=['name_image','name_video','num_frame','num_box','left','top','width','height'])\n",
    "    \n",
    "    #parsing XML and populating dataframe\n",
    "    tree = ET.parse(path_file_xml)\n",
    "    root = tree.getroot()\n",
    "    for image in tqdm(root.iter('image')):\n",
    "        name_file = image.attrib['file']\n",
    "        name_video = name_file.split('.')[0].rsplit(' ',1)[0]\n",
    "        num_frame = (int)(name_file.split('.')[0].rsplit(' ',1)[1])\n",
    "        \n",
    "        #if no box, box attributes are np.nan\n",
    "        if (len(image) == 0):\n",
    "            row_data = [name_file, name_video, num_frame, np.nan, np.nan, np.nan, np.nan, np.nan]\n",
    "            df.loc[len(df)] = row_data\n",
    "        else:\n",
    "            for box_num,box in enumerate(image):\n",
    "                box_attribs = box.attrib\n",
    "                row_data = [name_file, name_video, num_frame, box_num+1, box_attribs['left'], box_attribs['top'],box_attribs['width'],box_attribs['height']]\n",
    "                df.loc[len(df)] = row_data\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDictVideoBoxesPerFrameFromXML(path_file_xml):\n",
    "    \"\"\"\n",
    "    Returns a dictionary of form {video_name:[(frame_no,no_of_boxes),(frame_no+1,no_of_boxes),(frame_no+2,no_of_boxes)]} given the path of XML file\n",
    "    \"\"\"\n",
    "    \n",
    "    result = {}\n",
    "    #parsing XML and generating dictionary\n",
    "    tree = ET.parse(path_file_xml)\n",
    "    root = tree.getroot()\n",
    "    for image in tqdm(root.iter('image')):\n",
    "        name_file = image.attrib['file']\n",
    "        name_video = name_file.split('.')[0].rsplit(' ',1)[0]\n",
    "        num_frame = (int)(name_file.split('.')[0].rsplit(' ',1)[1])\n",
    "        num_boxes = len(image)\n",
    "        if name_video in result:\n",
    "            result[name_video].append((num_frame, num_boxes))\n",
    "        else:\n",
    "            result[name_video] = [(num_frame, num_boxes)]    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting number of boxes detected vs frame number "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(set([1, 2]), dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array({1,2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plotBoxesInFrames(videoName,zippedListFrameBox):\n",
    "    \"\"\"\n",
    "    Plots the number of boxes with frame given the name of the video and corresponding zippedlist of form [(frame_no,no_of_boxes),(frame_no+1,no_of_boxes)...]\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    videoName : Name of the video.\n",
    "    zippedListFrameBox : zipped list of the form [(frame_no,no_of_boxes),(frame_no+1,no_of_boxes),(frame_no+2,no_of_boxes)]\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    unzippedList = zip(*sorted(zippedListFrameBox))\n",
    "    x = np.array(unzippedList[0])\n",
    "    y = np.array(unzippedList[1])\n",
    "    \n",
    "    max_faces = max(y)\n",
    "    fig, (ax,ax2) = plt.subplots(nrows=2, sharex=True)\n",
    "\n",
    "#     extent = [0, x[-1],0,max_faces]\n",
    "    ax.imshow(y[np.newaxis,:], cmap=\"PuBu\", aspect=\"auto\")\n",
    "    ax.set_yticks([])\n",
    "#     ax.set_xlim(extent[0], extent[1])\n",
    "\n",
    "    ax2.plot(x,y)\n",
    "    ax2.legend([videoName],loc='center left',bbox_to_anchor=(1, 0.5))\n",
    "#     ax2.set_xlim(extent[0], extent[1])\n",
    "    plt.xlabel(\"Frame Number\")\n",
    "    plt.ylabel(\"Number of Faces Detected\")\n",
    "#     plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotit(video_file_name,dict_video_box_frame):\n",
    "    \"\"\"\n",
    "    Plots the number of boxes versus frames and also the heatmap to indicate the number of faces detected.\n",
    "    \"\"\"\n",
    "    \n",
    "    plotBoxesInFrames(video_file_name,dict_video_box_frame[video_file_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for analysis:\n",
    "    \n",
    "- Read XML, give complete analysis.\n",
    "- See folder, give the option of comparing the data in the different XML files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyseFromXML(path_xml_file):\n",
    "    \"\"\"\n",
    "    Parses the XML file given its path and plots the following statistics:\n",
    "    Histogram- No of faces for each frame, heatmap for number of faces detected in each frame.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    dictVideoBoxesPerFrame = getDictVideoBoxesPerFrameFromXML(path_xml_file) \n",
    "    listVideoNames=dictVideoBoxesPerFrame.keys() #to build options in dropdown.\n",
    "    video_file_name = widgets.Dropdown(options=listVideoNames,description='Video File:')\n",
    "    interactive_widgets = interactive(plotit,video_file_name=video_file_name, dict_video_box_frame=fixed(dictVideoBoxesPerFrame))\n",
    "    display(interactive_widgets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing ZFace data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44492it [00:00, 362823.79it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "aW50ZXJhY3RpdmUoY2hpbGRyZW49KERyb3Bkb3duKGRlc2NyaXB0aW9uPXUnVmlkZW8gRmlsZTonLCBvcHRpb25zPSgnVklERU9fMTUyNjQ3NTU5ODA1OCBkZWxpZ2h0XzIwMTgwNTE3XzExMTbigKY=\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "analyseFromXML('output_XML/zface-output.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing dlib data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "39016it [00:00, 139100.89it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21b7afd96b1946818f5c1f92410e90d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "aW50ZXJhY3RpdmUoY2hpbGRyZW49KERyb3Bkb3duKGRlc2NyaXB0aW9uPXUnVmlkZW8gRmlsZTonLCBvcHRpb25zPSgnVklERU9fMTUyNjQ3NTU3MTE4MSBib3JlZF8yMDE4MDUxNl8xMDQ1MjjigKY=\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "analyseFromXML('output_XML/dlib_hog_output_232_folders.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is helpful in understanding the continuity of face detection in a particular video.\n",
    "More plots that can be hepful:\n",
    "- Make a distinction wrt the environment and prepare some visualisations\n",
    "- Test and train performance has to be analysed separately if more data is added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further analysis to be done\n",
    "- Detect face inside the smaller bounding box.\n",
    "- See if averaging the pixels outside the box affects face detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp_path=random.sample(frames_list,1)[0]\n",
    "print(type(samp_path))\n",
    "image=cv2.imread(samp_path)\n",
    "gray=cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "rects=detector(gray,1)\n",
    "for i,rect in enumerate(rects):\n",
    "    shape = predictor(gray, rect)\n",
    "    shape = face_utils.shape_to_np(shape)\n",
    "    (x, y, w, h) = face_utils.rect_to_bb(rect)\n",
    "    cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 10)\n",
    "    cv2.circle(image, (x, y), 1, (0, 0, 255), -1)\n",
    "    # show the face number\n",
    "#     cv2.putText(image, \"Face #{}\".format(i + 1), (x - 10, y - 10),cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "plt.imshow(cv2.cvtColor(image,cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experiments\n",
    "- Face detection (2 hrs), try to visualise and understand exactly why HOG isn't working here, try to see the values obtained when similarity across each window is calculated.\n",
    "    - Try to detect face in cropped images.\n",
    "    - Try to detect face when background is of same color(try average, white, black)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
