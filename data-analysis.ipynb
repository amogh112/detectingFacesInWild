{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysing Face Detection on African data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import glob\n",
    "import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "from imutils import face_utils\n",
    "from skimage.feature import hog\n",
    "from skimage import data,exposure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib \n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import widgets, interactive, fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Path to Data Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_folder_bagamoyo_data = '/media/amogh/Stuff/CMU/datasets/bagamoyo_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Path to Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_folder_all_frames = path_folder_bagamoyo_data + '/bagamoyo_frames_all_in_one/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_folder_wise_frames = path_folder_bagamoyo_data + '/bagamoyo_frames_folder_wise/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Path to xml files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data-analysis.ipynb\t\t       train_dlib_detector.py\r\n",
      "Generate-xml-dlib.ipynb\t\t       training.xml\r\n",
      "README.md\t\t\t       ZFace label analysis.ipynb\r\n",
      "shape_predictor_68_face_landmarks.dat\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "comment: will change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_xml_file = 'training.xml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Absolute - Find the number of frames in total in which face is detected, and how many in each frame\n",
    "2. Check continuity - a visualisation for seeing which frames in the continuity.\n",
    "3. This can be done by writing a single script which does these things if an xml file is generated with the name of the image and the coordinates of the bounding boxes in it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parsing XML file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataframeFromXML(path_file_xml):\n",
    "    \"\"\"\n",
    "    Returns the dataframe(columns- videoName,frameNo,faceNo,left,right,width,height) from given xml file path holding bounding boxes for each frame.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path_file_xml : path of the XML file.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Pandas dataframe\n",
    "        Information about images and their boxes.\n",
    "    \"\"\"\n",
    "    \n",
    "    #make a new dataframe to store the data.\n",
    "    df = pd.DataFrame(columns=['name_image','name_video','num_frame','num_box','left','top','width','height'])\n",
    "    \n",
    "    #parsing XML and populating dataframe\n",
    "    tree = ET.parse(path_file_xml)\n",
    "    root = tree.getroot()\n",
    "    for image in tqdm(root.iter('image')):\n",
    "        name_file = image.attrib['file']\n",
    "        name_video = name_file.split('.')[0].rsplit(' ',1)[0]\n",
    "        num_frame = (int)(name_file.split('.')[0].rsplit(' ',1)[1])\n",
    "        \n",
    "        #if no box, box attributes are np.nan\n",
    "        if (len(image) == 0):\n",
    "            row_data = [name_file, name_video, num_frame, np.nan, np.nan, np.nan, np.nan, np.nan]\n",
    "            df.loc[len(df)] = row_data\n",
    "        else:\n",
    "            for box_num,box in enumerate(image):\n",
    "                box_attribs = box.attrib\n",
    "                row_data = [name_file, name_video, num_frame, box_num+1, box_attribs['left'], box_attribs['top'],box_attribs['width'],box_attribs['height']]\n",
    "                df.loc[len(df)] = row_data\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDictVideoBoxesPerFrameFromXML(path_file_xml):\n",
    "    \"\"\"\n",
    "    Returns a dictionary of form {video_name:[(frame_no,no_of_boxes),(frame_no+1,no_of_boxes),(frame_no+2,no_of_boxes)]} given the path of XML file\n",
    "    \"\"\"\n",
    "    \n",
    "    result = {}\n",
    "    #parsing XML and generating dictionary\n",
    "    tree = ET.parse(path_file_xml)\n",
    "    root = tree.getroot()\n",
    "    for image in tqdm(root.iter('image')):\n",
    "        name_file = image.attrib['file']\n",
    "        name_video = name_file.split('.')[0].rsplit(' ',1)[0]\n",
    "        num_frame = (int)(name_file.split('.')[0].rsplit(' ',1)[1])\n",
    "        num_boxes = len(image)\n",
    "        if name_video in result:\n",
    "            result[name_video].append((num_frame, num_boxes))\n",
    "        else:\n",
    "            result[name_video] = [(num_frame, num_boxes)]    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plotBoxesInFrames(videoName,zippedListFrameBox):\n",
    "    \"\"\"\n",
    "    Plots the number of boxes with frame given the name of the video and corresponding zippedlist of form [(frame_no,no_of_boxes),(frame_no+1,no_of_boxes)...]\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    videoName : Name of the video.\n",
    "    zippedListFrameBox : zipped list of the form [(frame_no,no_of_boxes),(frame_no+1,no_of_boxes),(frame_no+2,no_of_boxes)]\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    unzippedList = zip(*sorted(zippedListFrameBox))\n",
    "    x = np.array(unzippedList[0])\n",
    "    y = np.array(unzippedList[1])\n",
    "    \n",
    "    max_faces = max(y)\n",
    "    fig, (ax,ax2) = plt.subplots(nrows=2, sharex=True)\n",
    "\n",
    "#     extent = [0, x[-1],0,max_faces]\n",
    "    hm = ax.imshow(y[np.newaxis,:], cmap=\"PuBu\", aspect=\"auto\")\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "#     ax3 = fig.add_subplot(1,1,1)\n",
    "#     color_legend=ax.pcolor([x,y])\n",
    "#     ax.set_xlim(extent[0], extent[1])\n",
    "\n",
    "    ax2.plot(x,y)\n",
    "    ax2.legend([videoName],loc='center left',bbox_to_anchor=(1, 0.5))\n",
    "#     ax2.set_xlim(extent[0], extent[1])\n",
    "    plt.xlabel(\"Frame Number\")\n",
    "    plt.ylabel(\"Number of Faces Detected\")\n",
    "\n",
    "    fig.subplots_adjust(right=0.8)\n",
    "    cbar_ax = fig.add_axes([0.85, 0.5, 0.01, 0.4])\n",
    "    cbar = fig.colorbar(hm,pad=0.2,cax=cbar_ax,format='%1i',ticks=[i for i in range(20)])\n",
    "    cbar.set_label('# of faces', rotation=270)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plotit(video_file_name,dict_video_box_frame):\n",
    "    \"\"\"\n",
    "    Plots the number of boxes versus frames and also the heatmap to indicate the number of faces detected.\n",
    "    \"\"\"\n",
    "    \n",
    "    plotBoxesInFrames(video_file_name,dict_video_box_frame[video_file_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Functions for analysis:\n",
    "    \n",
    "- Read XML, give complete analysis.\n",
    "- See folder, give the option of comparing the data in the different XML files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### End to End functions (directly plot from XML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyseFromXML(path_xml_file):\n",
    "    \"\"\"\n",
    "    Parses the XML file given its path and plots the following statistics:\n",
    "    Histogram- No of faces for each frame, heatmap for number of faces detected in each frame.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    dictVideoBoxesPerFrame = getDictVideoBoxesPerFrameFromXML(path_xml_file) \n",
    "    listVideoNames=dictVideoBoxesPerFrame.keys() #to build options in dropdown.\n",
    "    video_file_name = widgets.Dropdown(options=listVideoNames,description='Video File:')\n",
    "    interactive_widgets = interactive(plotit,video_file_name=video_file_name, dict_video_box_frame=fixed(dictVideoBoxesPerFrame))\n",
    "    display(interactive_widgets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO Comparing and Analysing data from multiple sources\n",
    "- Show the options of the available xml files. Show the ones that can be compared. Have an option to show only the comparable ones by a simple toggle button"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Analysing ZFace data (show)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44492it [00:00, 316939.41it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc8a862eba814c0db44722153419c134",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "aW50ZXJhY3RpdmUoY2hpbGRyZW49KERyb3Bkb3duKGRlc2NyaXB0aW9uPXUnVmlkZW8gRmlsZTonLCBvcHRpb25zPSgnVklERU9fMTUyNjQ3NTU5ODA1OCBkZWxpZ2h0XzIwMTgwNTE3XzExMTbigKY=\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "analyseFromXML('output_XML/zface-output.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Analysing dlib data (show)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "39016it [00:00, 324905.77it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53228a68fa254f4898005177815702b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "aW50ZXJhY3RpdmUoY2hpbGRyZW49KERyb3Bkb3duKGRlc2NyaXB0aW9uPXUnVmlkZW8gRmlsZTonLCBvcHRpb25zPSgnVklERU9fMTUyNjQ3NTU3MTE4MSBib3JlZF8yMDE4MDUxNl8xMDQ1MjjigKY=\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "analyseFromXML('output_XML/dlib_hog_output_232_folders.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This is helpful in understanding the continuity of face detection in a particular video.\n",
    "More plots that can be hepful:\n",
    "- Make a distinction wrt the environment and prepare some visualisations\n",
    "- Test and train performance has to be analysed separately if more data is added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image specific analysis\n",
    "- Detect face inside the smaller bounding box.\n",
    "- See if averaging the pixels outside the box affects face detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions to get 10 random images and also see them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImages(path_xml_data, has_box=False, number_of_images=10, show_images=True):\n",
    "    #parsing XML\n",
    "    tree = ET.parse(path_xml_data)\n",
    "    root = tree.getroot()\n",
    "    list_file_names = []\n",
    "    for image_tag in root.iter('image'):\n",
    "        if not has_box:\n",
    "            if(len(image_tag) == 0): #no boxes\n",
    "                list_file_names.append(image_tag.attrib['file'])\n",
    "        else:\n",
    "            if (len(image_tag) != 0):\n",
    "                list_file_names.append(image_tag.attrib['file'])\n",
    "    \n",
    "    print(\"Number of images: \", len(list_file_names))\n",
    "    return list_file_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get 10 random images where face is not detected in an xml file.\n",
    "Find 10 images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp_path=random.sample(frames_list,1)[0]\n",
    "print(type(samp_path))\n",
    "image=cv2.imread(samp_path)\n",
    "gray=cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "rects=detector(gray,1)\n",
    "for i,rect in enumerate(rects):\n",
    "    shape = predictor(gray, rect)\n",
    "    shape = face_utils.shape_to_np(shape)\n",
    "    (x, y, w, h) = face_utils.rect_to_bb(rect)\n",
    "    cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 10)\n",
    "    cv2.circle(image, (x, y), 1, (0, 0, 255), -1)\n",
    "    # show the face number\n",
    "#     cv2.putText(image, \"Face #{}\".format(i + 1), (x - 10, y - 10),cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "plt.imshow(cv2.cvtColor(image,cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO Experiments\n",
    "- Face detection (2 hrs), try to visualise and understand exactly why HOG isn't working here, try to see the values obtained when similarity across each window is calculated.\n",
    "    - Try to detect face in cropped images.\n",
    "    - Try to detect face when background is of same color(try average, white, black).\n",
    "- See the values in terms of HOG detector, what is going wrong. Read about it and see which visualisations are wrong."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
